#!/usr/bin/env python3
"""
Comprehensive Test Suite for Unified Dashboard

This test suite validates all integrated features:
- Light initialization with fallbacks
- Service management functionality
- Backend monitoring
- Real-time metrics
- MCP protocol compatibility
- WebSocket real-time updates
- Bucket VFS operations
- Template system integration
"""

import asyncio
import json
import logging
import os
import tempfile
import unittest
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock
import sys

# Add the current directory to Python path for imports
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

try:
    from unified_comprehensive_dashboard import UnifiedComprehensiveDashboard
    DASHBOARD_AVAILABLE = True
except ImportError as e:
    print(f"âŒ Failed to import dashboard: {e}")
    DASHBOARD_AVAILABLE = False

# Configure logging for tests
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TestUnifiedComprehensiveDashboard(unittest.IsolatedAsyncioTestCase):
    """Test cases for the unified comprehensive dashboard."""
    
    async def asyncSetUp(self):
        """Set up test environment."""
        if not DASHBOARD_AVAILABLE:
            self.skipTest("Dashboard not available for testing")
        
        # Create temporary directory for testing
        self.temp_dir = tempfile.mkdtemp()
        self.test_config = {
            'host': '127.0.0.1',
            'port': 8081,  # Use different port for testing
            'data_dir': self.temp_dir,
            'debug': True,
            'websocket_enabled': True,
            'log_streaming': True
        }
        
        # Initialize dashboard
        self.dashboard = UnifiedComprehensiveDashboard(self.test_config)
        
        logger.info(f"âœ… Test environment set up with temp dir: {self.temp_dir}")
    
    async def asyncTearDown(self):
        """Clean up test environment."""
        # Clean up temporary directory
        import shutil
        if hasattr(self, 'temp_dir') and Path(self.temp_dir).exists():
            shutil.rmtree(self.temp_dir)
        logger.info("ğŸ§¹ Test environment cleaned up")
    
    async def test_dashboard_initialization(self):
        """Test dashboard initialization with comprehensive features."""
        logger.info("ğŸ§ª Testing dashboard initialization...")
        
        # Test basic initialization
        self.assertIsNotNone(self.dashboard)
        self.assertEqual(self.dashboard.host, '127.0.0.1')
        self.assertEqual(self.dashboard.port, 8081)
        self.assertTrue(self.dashboard.websocket_enabled)
        
        # Test state directories creation
        self.assertTrue(self.dashboard.data_dir.exists())
        self.assertTrue(self.dashboard.buckets_dir.exists())
        self.assertTrue(self.dashboard.backends_dir.exists())
        self.assertTrue(self.dashboard.services_dir.exists())
        
        # Test FastAPI app creation
        self.assertIsNotNone(self.dashboard.app)
        self.assertEqual(self.dashboard.app.title, "IPFS Kit - Unified Comprehensive Dashboard")
        
        # Test MCP tools registration
        self.assertIsInstance(self.dashboard.mcp_tools, dict)
        self.assertGreater(len(self.dashboard.mcp_tools), 0)
        
        logger.info("âœ… Dashboard initialization test passed")
    
    async def test_light_initialization_fallbacks(self):
        """Test that light initialization works with import fallbacks."""
        logger.info("ğŸ§ª Testing light initialization fallbacks...")
        
        # Test that dashboard works even when optional components fail
        self.assertIsNotNone(self.dashboard.unified_bucket_interface)
        
        # Test fallback behavior for missing components
        if hasattr(self.dashboard, 'ipfs_api') and self.dashboard.ipfs_api is None:
            logger.info("âœ… IPFS API fallback working correctly")
        
        if hasattr(self.dashboard, 'bucket_manager') and self.dashboard.bucket_manager is None:
            logger.info("âœ… Bucket manager fallback working correctly")
        
        logger.info("âœ… Light initialization fallbacks test passed")
    
    async def test_mcp_protocol_compatibility(self):
        """Test MCP protocol endpoint compatibility."""
        logger.info("ğŸ§ª Testing MCP protocol compatibility...")
        
        # Test MCP initialization
        init_result = await self.dashboard.app.post("/mcp/initialize")
        # Note: This would need actual HTTP client testing in real scenario
        
        # Test MCP tools registration
        required_tools = [
            "list_files", "read_file", "write_file",
            "daemon_status", "start_service", "stop_service",
            "list_backends", "backend_health", "list_buckets",
            "create_bucket", "list_pins", "pin_content",
            "system_metrics", "peer_info"
        ]
        
        for tool in required_tools:
            self.assertIn(tool, self.dashboard.mcp_tools)
            tool_def = self.dashboard.mcp_tools[tool]
            self.assertIn("name", tool_def)
            self.assertIn("description", tool_def)
            self.assertIn("input_schema", tool_def)
        
        logger.info("âœ… MCP protocol compatibility test passed")
    
    async def test_mcp_tool_execution(self):
        """Test MCP tool execution."""
        logger.info("ğŸ§ª Testing MCP tool execution...")
        
        # Test system metrics tool
        metrics_result = await self.dashboard._execute_mcp_tool("system_metrics", {})
        self.assertIsInstance(metrics_result, dict)
        self.assertIn("timestamp", metrics_result)
        
        # Test daemon status tool
        status_result = await self.dashboard._execute_mcp_tool("daemon_status", {})
        self.assertIsInstance(status_result, dict)
        
        # Test list buckets tool
        buckets_result = await self.dashboard._execute_mcp_tool("list_buckets", {})
        self.assertIsInstance(buckets_result, dict)
        
        # Test file operations
        # First create a test file
        test_file = Path(self.temp_dir) / "test.txt"
        test_content = "Test content for MCP file operations"
        
        # Test write file
        write_result = await self.dashboard._execute_mcp_tool("write_file", {
            "path": str(test_file),
            "content": test_content
        })
        self.assertTrue(write_result.get("success", False))
        
        # Test read file
        read_result = await self.dashboard._execute_mcp_tool("read_file", {
            "path": str(test_file)
        })
        self.assertEqual(read_result.get("content"), test_content)
        
        # Test list files
        list_result = await self.dashboard._execute_mcp_tool("list_files", {
            "path": str(self.temp_dir)
        })
        self.assertIn("files", list_result)
        file_names = [f["name"] for f in list_result["files"]]
        self.assertIn("test.txt", file_names)
        
        logger.info("âœ… MCP tool execution test passed")
    
    async def test_service_management(self):
        """Test service management functionality."""
        logger.info("ğŸ§ª Testing service management...")
        
        # Test service status retrieval
        service_status = await self.dashboard._get_service_status()
        self.assertIsInstance(service_status, dict)
        self.assertIn("mcp_server", service_status)
        self.assertIn("dashboard", service_status)
        
        # Test that MCP server and dashboard show as running
        self.assertEqual(service_status["mcp_server"]["status"], "running")
        self.assertEqual(service_status["dashboard"]["status"], "running")
        
        # Test service management tools (these would normally interact with real services)
        # For testing, we check that the methods exist and return proper structure
        
        logger.info("âœ… Service management test passed")
    
    async def test_backend_monitoring(self):
        """Test backend monitoring functionality."""
        logger.info("ğŸ§ª Testing backend monitoring...")
        
        # Test backend status retrieval
        backend_status = await self.dashboard._get_backend_status()
        self.assertIsInstance(backend_status, dict)
        self.assertIn("backends", backend_status)
        self.assertIn("summary", backend_status)
        
        # Test backend health monitoring
        backend_health = await self.dashboard._get_backend_health()
        self.assertIsInstance(backend_health, dict)
        self.assertIn("timestamp", backend_health)
        self.assertIn("backends", backend_health)
        
        # Test backend performance metrics
        performance = await self.dashboard._get_backend_performance("test_backend")
        self.assertIsInstance(performance, dict)
        self.assertIn("backend", performance)
        self.assertIn("metrics", performance)
        
        logger.info("âœ… Backend monitoring test passed")
    
    async def test_real_time_metrics(self):
        """Test real-time metrics collection."""
        logger.info("ğŸ§ª Testing real-time metrics...")
        
        # Test system metrics collection
        metrics = await self.dashboard._get_system_metrics()
        self.assertIsInstance(metrics, dict)
        self.assertIn("timestamp", metrics)
        self.assertIn("cpu", metrics)
        self.assertIn("memory", metrics)
        self.assertIn("disk", metrics)
        
        # Test system overview
        overview = await self.dashboard._get_system_overview()
        self.assertIsInstance(overview, dict)
        self.assertIn("timestamp", overview)
        self.assertIn("uptime", overview)
        self.assertIn("system", overview)
        
        logger.info("âœ… Real-time metrics test passed")
    
    async def test_websocket_manager(self):
        """Test WebSocket functionality."""
        logger.info("ğŸ§ª Testing WebSocket manager...")
        
        if hasattr(self.dashboard, 'websocket_manager'):
            websocket_manager = self.dashboard.websocket_manager
            
            # Test initial state
            self.assertEqual(len(websocket_manager.active_connections), 0)
            
            # Test broadcast with no connections (should not error)
            await websocket_manager.broadcast({"type": "test", "data": "test_data"})
            
            logger.info("âœ… WebSocket manager test passed")
        else:
            logger.info("âš ï¸ WebSocket manager not available, skipping test")
    
    async def test_log_streaming(self):
        """Test log streaming functionality."""
        logger.info("ğŸ§ª Testing log streaming...")
        
        if hasattr(self.dashboard, 'log_handler'):
            log_handler = self.dashboard.log_handler
            
            # Generate a test log entry
            test_logger = logging.getLogger("test_component")
            test_logger.info("Test log message for streaming")
            
            # Retrieve logs
            logs = log_handler.get_logs(component="test_component", limit=10)
            self.assertIsInstance(logs, list)
            
            if logs:
                log_entry = logs[-1]  # Get the most recent log
                self.assertIn("message", log_entry)
                self.assertIn("timestamp", log_entry)
                self.assertIn("level", log_entry)
                self.assertIn("component", log_entry)
            
            logger.info("âœ… Log streaming test passed")
        else:
            logger.info("âš ï¸ Log streaming not available, skipping test")
    
    async def test_bucket_vfs_operations(self):
        """Test bucket VFS operations."""
        logger.info("ğŸ§ª Testing bucket VFS operations...")
        
        # Test bucket interface availability
        self.assertIsNotNone(self.dashboard.unified_bucket_interface)
        
        # Test bucket listing
        try:
            bucket_result = await self.dashboard.unified_bucket_interface.list_backend_buckets()
            self.assertIsInstance(bucket_result, dict)
            logger.info("âœ… Bucket interface responding correctly")
        except Exception as e:
            logger.info(f"âš ï¸ Bucket interface error (expected in test environment): {e}")
        
        # Test bucket directory structure
        self.assertTrue(self.dashboard.buckets_dir.exists())
        
        logger.info("âœ… Bucket VFS operations test passed")
    
    async def test_state_directory_management(self):
        """Test ~/.ipfs_kit/ state directory management."""
        logger.info("ğŸ§ª Testing state directory management...")
        
        # Test that all required directories are created
        required_dirs = [
            self.dashboard.buckets_dir,
            self.dashboard.backends_dir,
            self.dashboard.services_dir,
            self.dashboard.config_dir,
            self.dashboard.logs_dir,
            self.dashboard.program_state_dir,
            self.dashboard.pins_dir
        ]
        
        for dir_path in required_dirs:
            self.assertTrue(dir_path.exists(), f"Directory {dir_path} should exist")
            self.assertTrue(dir_path.is_dir(), f"Path {dir_path} should be a directory")
        
        # Test writing and reading from state directories
        test_config_file = self.dashboard.config_dir / "test_config.json"
        test_config_data = {"test": "data", "timestamp": "2024-01-01"}
        
        with open(test_config_file, 'w') as f:
            json.dump(test_config_data, f)
        
        with open(test_config_file, 'r') as f:
            loaded_data = json.load(f)
        
        self.assertEqual(loaded_data, test_config_data)
        
        logger.info("âœ… State directory management test passed")
    
    async def test_comprehensive_feature_integration(self):
        """Test that all comprehensive features are integrated."""
        logger.info("ğŸ§ª Testing comprehensive feature integration...")
        
        # Test that all major feature categories are available
        feature_checks = {
            "MCP Tools": len(self.dashboard.mcp_tools) > 10,
            "FastAPI App": self.dashboard.app is not None,
            "State Directories": self.dashboard.data_dir.exists(),
            "Bucket Interface": self.dashboard.unified_bucket_interface is not None,
            "WebSocket Manager": hasattr(self.dashboard, 'websocket_manager'),
            "Log Handler": hasattr(self.dashboard, 'log_handler') if self.dashboard.log_streaming else True
        }
        
        for feature, available in feature_checks.items():
            self.assertTrue(available, f"Feature '{feature}' should be available")
            logger.info(f"âœ… {feature}: Available")
        
        # Test that the dashboard has all expected methods
        expected_methods = [
            '_get_system_overview',
            '_get_system_metrics',
            '_get_service_status',
            '_get_backend_status',
            '_get_backend_health',
            '_execute_mcp_tool',
            '_start_service',
            '_stop_service'
        ]
        
        for method_name in expected_methods:
            self.assertTrue(hasattr(self.dashboard, method_name), 
                          f"Method '{method_name}' should be available")
        
        logger.info("âœ… Comprehensive feature integration test passed")
    
    async def test_error_handling_and_fallbacks(self):
        """Test error handling and fallback mechanisms."""
        logger.info("ğŸ§ª Testing error handling and fallbacks...")
        
        # Test MCP tool execution with invalid tool
        invalid_result = await self.dashboard._execute_mcp_tool("invalid_tool", {})
        self.assertIn("error", invalid_result)
        
        # Test MCP tool execution with invalid parameters
        invalid_params_result = await self.dashboard._execute_mcp_tool("read_file", {})
        self.assertIn("error", invalid_params_result)
        
        # Test file operations with invalid paths
        invalid_file_result = await self.dashboard._execute_mcp_tool("read_file", {
            "path": "/nonexistent/file.txt"
        })
        self.assertIn("error", invalid_file_result)
        
        logger.info("âœ… Error handling and fallbacks test passed")


class TestDashboardIntegration(unittest.TestCase):
    """Integration tests for dashboard functionality."""
    
    def setUp(self):
        """Set up integration test environment."""
        if not DASHBOARD_AVAILABLE:
            self.skipTest("Dashboard not available for testing")
    
    def test_dashboard_import(self):
        """Test that dashboard can be imported successfully."""
        logger.info("ğŸ§ª Testing dashboard import...")
        
        self.assertTrue(DASHBOARD_AVAILABLE, "Dashboard should be importable")
        
        # Test that the main class is available
        from unified_comprehensive_dashboard import UnifiedComprehensiveDashboard
        self.assertIsNotNone(UnifiedComprehensiveDashboard)
        
        logger.info("âœ… Dashboard import test passed")
    
    def test_dashboard_instantiation(self):
        """Test dashboard can be instantiated with default config."""
        logger.info("ğŸ§ª Testing dashboard instantiation...")
        
        # Test with default config
        dashboard = UnifiedComprehensiveDashboard()
        self.assertIsNotNone(dashboard)
        
        # Test with custom config
        custom_config = {
            'host': '0.0.0.0',
            'port': 9000,
            'debug': True
        }
        dashboard_custom = UnifiedComprehensiveDashboard(custom_config)
        self.assertEqual(dashboard_custom.host, '0.0.0.0')
        self.assertEqual(dashboard_custom.port, 9000)
        self.assertTrue(dashboard_custom.debug)
        
        logger.info("âœ… Dashboard instantiation test passed")


async def run_comprehensive_tests():
    """Run all comprehensive tests."""
    logger.info("ğŸš€ Starting Comprehensive Dashboard Test Suite")
    logger.info("=" * 60)
    
    # Run async tests
    if DASHBOARD_AVAILABLE:
        suite = unittest.TestLoader().loadTestsFromTestCase(TestUnifiedComprehensiveDashboard)
        runner = unittest.TextTestRunner(verbosity=2)
        result = runner.run(suite)
        
        # Run sync tests
        suite_sync = unittest.TestLoader().loadTestsFromTestCase(TestDashboardIntegration)
        result_sync = runner.run(suite_sync)
        
        # Summary
        total_tests = result.testsRun + result_sync.testsRun
        total_failures = len(result.failures) + len(result_sync.failures)
        total_errors = len(result.errors) + len(result_sync.errors)
        
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š Test Summary:")
        logger.info(f"   Total Tests: {total_tests}")
        logger.info(f"   Passed: {total_tests - total_failures - total_errors}")
        logger.info(f"   Failed: {total_failures}")
        logger.info(f"   Errors: {total_errors}")
        
        if total_failures == 0 and total_errors == 0:
            logger.info("ğŸ‰ ALL TESTS PASSED! Dashboard integration successful!")
            return True
        else:
            logger.error("âŒ Some tests failed. Check the output above for details.")
            return False
    else:
        logger.error("âŒ Dashboard not available for testing")
        return False


def main():
    """Main test runner."""
    print("ğŸ§ª IPFS Kit - Unified Comprehensive Dashboard Test Suite")
    print("Testing all integrated features and functionality")
    print()
    
    # Check if dashboard is available
    if not DASHBOARD_AVAILABLE:
        print("âŒ Cannot run tests: Dashboard not available")
        print("ğŸ’¡ Make sure you're in the correct directory and all dependencies are installed")
        return False
    
    # Run tests
    success = asyncio.run(run_comprehensive_tests())
    
    if success:
        print("\nğŸ¯ INTEGRATION SUCCESS!")
        print("âœ… All comprehensive features are working correctly")
        print("âœ… Light initialization with fallbacks working")
        print("âœ… MCP protocol compatibility confirmed")
        print("âœ… Service management functional")
        print("âœ… Backend monitoring operational")
        print("âœ… Real-time metrics working")
        print("âœ… WebSocket support enabled")
        print("âœ… Bucket VFS operations ready")
        print("âœ… State directory management working")
        print("\nğŸš€ The unified comprehensive dashboard is ready for use!")
    else:
        print("\nâŒ Integration tests failed")
        print("ğŸ’¡ Review the test output to identify and fix issues")
    
    return success


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
